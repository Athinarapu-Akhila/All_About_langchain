{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "STEP 1: Install required libraries"
      ],
      "metadata": {
        "id": "xjGthDEjM33j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pfJ67FL7M1ca",
        "outputId": "a5127643-5e4d-4461-cadd-6bd31d45a77b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.6/330.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m711.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-exporter-otlp-proto-common==1.38.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-proto==1.38.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-sdk~=1.38.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-core langchain-community \\\n",
        "langchain-huggingface langchain-text-splitters \\\n",
        "pypdf sentence_transformers chromadb \\\n",
        "huggingface_hub transformers accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2: Import libraries"
      ],
      "metadata": {
        "id": "0v00BNXsNDMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ7KbGbQNKT6",
        "outputId": "8f82c277-9c22-4621-a9fb-c16cb7ba9588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 3: Upload document (PDF or TXT)\n"
      ],
      "metadata": {
        "id": "p-pkbRFTNSTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "if filename.endswith(\".pdf\"):\n",
        "    loader = PyPDFLoader(filename)\n",
        "elif filename.endswith(\".txt\"):\n",
        "    loader = TextLoader(filename)\n",
        "else:\n",
        "    raise ValueError(\"Only PDF or TXT files supported\")\n",
        "\n",
        "documents = loader.load()\n",
        "print(f\"Loaded {len(documents)} document(s)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "ux7Rgh6GNazj",
        "outputId": "7797e960-c5db-4971-bb0b-0e875cc07ad1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d301b8ed-2f0c-4190-b590-43aa1d1f260b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d301b8ed-2f0c-4190-b590-43aa1d1f260b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Major Project phase-2 Documentation-new-1.pdf to Major Project phase-2 Documentation-new-1.pdf\n",
            "Loaded 53 document(s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 4: Split document into chunks"
      ],
      "metadata": {
        "id": "TpusJJFFOa9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eploring Fixed Size chunking"
      ],
      "metadata": {
        "id": "MZBsVoWr4-ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=150\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_documents(documents)\n",
        "print(f\"Split into {len(texts)} chunks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DLCA9F7OVAq",
        "outputId": "3b68d890-57cd-4488-b061-d3ec0dc6a461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split into 108 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "add347cc"
      },
      "source": [
        "### Exploring Semantic Chunking\n",
        "\n",
        "Let's try a different chunking strategy focusing on semantic coherence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a5d1bd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aabb547-9897-4845-f693-75913486a7b6"
      },
      "source": [
        "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
        "\n",
        "# Initialize the semantic text splitter\n",
        "semantic_text_splitter = SentenceTransformersTokenTextSplitter(\n",
        "    chunk_overlap=0,\n",
        "    tokens_per_chunk=256 # Adjust as needed, based on typical sentence length and model limits\n",
        ")\n",
        "\n",
        "# Split the documents using the semantic splitter\n",
        "semantic_texts = semantic_text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Split into {len(semantic_texts)} semantic chunks\")\n",
        "\n",
        "# Optionally, you can now create a new vector database with these semantic chunks\n",
        "# semantic_vectordb = Chroma.from_documents(\n",
        "#     documents=semantic_texts,\n",
        "#     embedding=embeddings\n",
        "# )\n",
        "# semantic_retriever = semantic_vectordb.as_retriever()\n",
        "# print(\"Semantic Vector DB ready\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split into 95 semantic chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 5: Create embeddings + vector database\n"
      ],
      "metadata": {
        "id": "YN9dejyrOhRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=semantic_texts, #changethe variable name associated with which chunking method u have used\n",
        "    embedding=embeddings\n",
        ")\n",
        "\n",
        "retriever = vectordb.as_retriever()\n",
        "print(\"Vector DB ready with semantic chunks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SstrdlloOnjr",
        "outputId": "8047a2bb-d4e7-453a-817b-f679f18bede0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector DB ready with semantic chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 6: Load local LLM (FLAN-T5)"
      ],
      "metadata": {
        "id": "aHh0CnY6OsRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "print(\"LLM loaded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnNalG_7OzFT",
        "outputId": "99811dd6-ca03-4fd3-84e5-f4c1b1d8ce41"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 7: Create RAG chain\n"
      ],
      "metadata": {
        "id": "EKVNYAs9O7x2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Use the following context to answer the question. If the answer is not in the context, say \"I don't know.\" Don't try to make up an answer.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retriever | format_docs,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"RAG chain ready with updated prompt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDBtC7SeOzIY",
        "outputId": "5617517c-ce02-4663-930b-817b1f1f6dd9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG chain ready with updated prompt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 8: Ask user questions"
      ],
      "metadata": {
        "id": "0BmZMmanPBZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    query = input(\"\\nAsk a question (type 'exit' to stop): \")\n",
        "\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    answer = rag_chain.invoke(query)\n",
        "    print(\"\\nAnswer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUDVCdD2PGmx",
        "outputId": "59c370fc-a0af-4067-ee60-80b537afe4b2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ask a question (type 'exit' to stop): what isuse of metamask?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (726 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Answer: It is a cryptocurrency wallet that enables users to store ether and other ethereum request for comments 20 tokens. it is a browser plugin that serves as an ethereum wallet, and is installed like any other browser plugin. it can also be used to interact with decentralized applications ( dapp ). metamask is a reliable tool with an easy - to - navigate user interface and consistent customer support. metamask helps you to access your funds without any hassle, as it does not require you to manage private keys at each and every transaction that you undertake. instead, it automatically signs all transactions and pops up a confirmation window when you mak e a payment. however, you have to remember a set of words that will prove your identity. this wallet is widely used by people who want to make secure cryptocurrency payments. it is safe to use because it works with the ethereum blockchain. this eliminates the need to download entire blockchains, minimizes the risk of malware, and protects your personal information.\n",
            "\n",
            "Ask a question (type 'exit' to stop): SHA 256 algorithm return the otuputs in how many bits?\n",
            "\n",
            "Answer: 512\n",
            "\n",
            "Ask a question (type 'exit' to stop): which algorithm is faster? MD5 or SHA?\n",
            "\n",
            "Answer: MD5\n",
            "\n",
            "Ask a question (type 'exit' to stop): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c648392b"
      },
      "source": [
        "### Understanding Semantic Chunking Methods\n",
        "\n",
        "Semantic chunking aims to split text into coherent, meaningful segments, ensuring that each chunk contains a complete thought or idea. This is often preferred over fixed-size chunking (like `RecursiveCharacterTextSplitter`) because it helps preserve the context that an LLM needs to understand and answer questions accurately.\n",
        "\n",
        "Here are some common semantic chunking methods:\n",
        "\n",
        "1.  **Sentence-based Chunking:**\n",
        "    *   **How it splits:** The simplest form, where each sentence forms a chunk. It's granular but might break apart ideas that span multiple sentences if the context window is very small.\n",
        "    *   **Pros:** Preserves sentence integrity, easy to implement.\n",
        "    *   **Cons:** Can create many small chunks, potentially losing broader context.\n",
        "\n",
        "2.  **Paragraph-based Chunking:**\n",
        "    *   **How it splits:** Splits documents into chunks based on paragraph breaks. This often results in more semantically coherent chunks than sentence-based.\n",
        "    *   **Pros:** Generally good at keeping related ideas together.\n",
        "    *   **Cons:** Paragraphs can be very long or very short, leading to uneven chunk sizes.\n",
        "\n",
        "3.  **Recursive Splitting with Semantic Boundaries:**\n",
        "    *   **How it splits:** This method is more advanced. It starts with large chunks (e.g., by paragraph or header) and then recursively splits them into smaller pieces if they exceed a certain size. The key difference from plain `RecursiveCharacterTextSplitter` is that it might use language models or embedding similarity to identify natural break points (e.g., points of lowest semantic similarity) within a larger chunk.\n",
        "    *   **Pros:** Flexible, can adapt to different document structures, aims for optimal semantic coherence.\n",
        "    *   **Cons:** More complex to implement, can be computationally intensive if involving LLMs or embeddings for splitting decisions.\n",
        "\n",
        "4.  **Token-based Splitting with Semantic Awareness (e.g., `SentenceTransformersTokenTextSplitter`):**\n",
        "    *   **How it splits:** This method, like the one we used (`SentenceTransformersTokenTextSplitter`), is designed to create chunks that respect token limits while trying to maintain semantic integrity. It often leverages underlying Sentence Transformer models to understand sentence boundaries and create chunks based on a maximum number of tokens, prioritizing full sentences or sub-sentences.\n",
        "    *   **Pros:** Efficiently handles token limits, good balance between chunk size and semantic coherence, directly compatible with embedding models.\n",
        "    *   **Cons:** Still primarily driven by token count, so it might not always perfectly capture very long, complex semantic units across strict token boundaries.\n",
        "\n",
        "### How `SentenceTransformersTokenTextSplitter` Splits Our Data\n",
        "\n",
        "In our notebook, the `SentenceTransformersTokenTextSplitter` was configured with `tokens_per_chunk=256`. This means it attempts to create chunks that are roughly 256 tokens long, while trying to respect sentence boundaries. Let's inspect some of these semantic chunks to see how they differ from the fixed-size chunks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4697000",
        "outputId": "c75e802f-d75b-44a0-ba73-1f2077ea51c9"
      },
      "source": [
        "# Print the first 3 semantic chunks\n",
        "print(\"First 3 Semantic Chunks (using SentenceTransformersTokenTextSplitter):\")\n",
        "for i, chunk in enumerate(semantic_texts[:3]):\n",
        "    print(f\"\\n--- Semantic Chunk {i+1} ---\")\n",
        "    print(f\"Length: {len(chunk.page_content.split())} words / {len(chunk.page_content)} characters\")\n",
        "    print(chunk.page_content)\n",
        "\n",
        "# For comparison, print the first 3 fixed-size chunks (if available)\n",
        "if 'texts' in locals() and len(texts) > 0:\n",
        "    print(\"\\n\\nFirst 3 Fixed-Size Chunks (using RecursiveCharacterTextSplitter):\")\n",
        "    for i, chunk in enumerate(texts[:3]):\n",
        "        print(f\"\\n--- Fixed-Size Chunk {i+1} ---\")\n",
        "        print(f\"Length: {len(chunk.page_content.split())} words / {len(chunk.page_content)} characters\")\n",
        "        print(chunk.page_content)\n",
        "else:\n",
        "    print(\"\\nFixed-size chunks (variable 'texts') not available for comparison.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 Semantic Chunks (using SentenceTransformersTokenTextSplitter):\n",
            "\n",
            "--- Semantic Chunk 1 ---\n",
            "Length: 115 words / 757 characters\n",
            "i a project phase - ii report on decentralized smart contract certificate system using ethereum blockchain technology submitted to the department of computer science & engineering, gnits in the partial fulfillment of the academic requirement for the award of b. tech ( cse ) under jntuh by mothukuri ashritha ( 19251a0598 ) saieni alankruthi ( 19251a05b0 ) thummanapalli preethi ( 19251a05b8 ) akhila athinarapu ( 20255a0507 ) under the guidance of dr. raghavender k. v. associate professor, department of cse department of computer science and engineering g. narayanamma institute of technology & science ( autonomous ) ( for women ) shaikpet, hyderabad - 500104. affiliated to jawaharlal nehru technological university hyderabad hyderabad 500085 may, 2023\n",
            "\n",
            "--- Semantic Chunk 2 ---\n",
            "Length: 158 words / 1036 characters\n",
            "ii g. narayanamma institute of technology & science ( autonomous ) ( for women ) shaikpet, hyderabad 500104. department of computer science and engineering certificate this is to certify that the project report on “ decentralized smart contract certificate system using ethereum blockchain technology ” is a bonafide work carried out by mothukuri ashritha ( 19251a0598 ), saieni alankruthi ( 19251a05b0 ), thummanapalli preethi ( 19251a05b8 ), akhila athinarapu ( 20255a0507 ) in the partial fulfillment for th e award of b. tech degree in computer science & engineering, g. narayanamma institute of technology & science, shaikpet, hyderabad, affiliated to jawaharlal nehru technological university, hyderabad under our guidance and supervision. the results embodied in the project work have not been submitted to any other university or institute for the award of any degree or diploma. internal guide head of the department dr. raghavender. k v dr. m. seetha associate professor professor and head, department of cse external examiner\n",
            "\n",
            "--- Semantic Chunk 3 ---\n",
            "Length: 184 words / 1166 characters\n",
            "iii acknowledgements we would like to express our sincere thanks to dr k ramesh reddy, principal, gnits, for providing the working facilities in the college. our sincere thanks and gratitude to dr. m seetha, professor & head, department of cse, gnits for all the timely support and valuable suggestions during the period of our project. we are extremely thankful to dr. n. kalyan i, professor, overall project coo rdinator, department of cse, gnits for all the valuable suggestions and guidance during the period of our project. we are extremely thankful to mrs. j. srilatha, mrs. g. sandhya, a ssistant professor ( s ), department of cse, gnits, project coordinators for their encouragement and support throughout the project. we are extremely thankful and indebted to our internal guide, dr. raghavender. k v, associate professor, department of cse, gnits for his / her constant guidance, encouragement and moral support throughout the project. finally, we are extremely thankful to all the faculty members and staff of cse department who helped us directly or indirectly, parents and friends for their cooperation in completing the project work. mothukuri ashrith\n",
            "\n",
            "\n",
            "First 3 Fixed-Size Chunks (using RecursiveCharacterTextSplitter):\n",
            "\n",
            "--- Fixed-Size Chunk 1 ---\n",
            "Length: 115 words / 757 characters\n",
            "i a project phase - ii report on decentralized smart contract certificate system using ethereum blockchain technology submitted to the department of computer science & engineering, gnits in the partial fulfillment of the academic requirement for the award of b. tech ( cse ) under jntuh by mothukuri ashritha ( 19251a0598 ) saieni alankruthi ( 19251a05b0 ) thummanapalli preethi ( 19251a05b8 ) akhila athinarapu ( 20255a0507 ) under the guidance of dr. raghavender k. v. associate professor, department of cse department of computer science and engineering g. narayanamma institute of technology & science ( autonomous ) ( for women ) shaikpet, hyderabad - 500104. affiliated to jawaharlal nehru technological university hyderabad hyderabad 500085 may, 2023\n",
            "\n",
            "--- Fixed-Size Chunk 2 ---\n",
            "Length: 158 words / 1036 characters\n",
            "ii g. narayanamma institute of technology & science ( autonomous ) ( for women ) shaikpet, hyderabad 500104. department of computer science and engineering certificate this is to certify that the project report on “ decentralized smart contract certificate system using ethereum blockchain technology ” is a bonafide work carried out by mothukuri ashritha ( 19251a0598 ), saieni alankruthi ( 19251a05b0 ), thummanapalli preethi ( 19251a05b8 ), akhila athinarapu ( 20255a0507 ) in the partial fulfillment for th e award of b. tech degree in computer science & engineering, g. narayanamma institute of technology & science, shaikpet, hyderabad, affiliated to jawaharlal nehru technological university, hyderabad under our guidance and supervision. the results embodied in the project work have not been submitted to any other university or institute for the award of any degree or diploma. internal guide head of the department dr. raghavender. k v dr. m. seetha associate professor professor and head, department of cse external examiner\n",
            "\n",
            "--- Fixed-Size Chunk 3 ---\n",
            "Length: 184 words / 1166 characters\n",
            "iii acknowledgements we would like to express our sincere thanks to dr k ramesh reddy, principal, gnits, for providing the working facilities in the college. our sincere thanks and gratitude to dr. m seetha, professor & head, department of cse, gnits for all the timely support and valuable suggestions during the period of our project. we are extremely thankful to dr. n. kalyan i, professor, overall project coo rdinator, department of cse, gnits for all the valuable suggestions and guidance during the period of our project. we are extremely thankful to mrs. j. srilatha, mrs. g. sandhya, a ssistant professor ( s ), department of cse, gnits, project coordinators for their encouragement and support throughout the project. we are extremely thankful and indebted to our internal guide, dr. raghavender. k v, associate professor, department of cse, gnits for his / her constant guidance, encouragement and moral support throughout the project. finally, we are extremely thankful to all the faculty members and staff of cse department who helped us directly or indirectly, parents and friends for their cooperation in completing the project work. mothukuri ashrith\n"
          ]
        }
      ]
    }
  ]
}